#! /usr/bin/env bash

set -o pipefail
set -o errexit
set -o nounset

THIS_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
cd ${THIS_DIR}/../..

export WDM_DEPENDENCY_FILE=${THIS_DIR}/../wdm/gpu-operator.yml

_warning() {
    fname="$1"
    msg="$2"

    DEST_DIR="${ARTIFACT_DIR}/_WARNING/"
    mkdir -p "$DEST_DIR"
    echo "$msg" > "${DEST_DIR}/$fname"

    echo "WARNING: $msg"
}

_expected_fail() {
    # mark the last toolbox step as an expected fail (for clearer
    # parsing/display in ci-dashboard)
    # eg: if cluster doesn't have NFD labels (expected fail), deploy NFD
    # eg: if cluster doesn't have GPU nodes (expected fail), scale up with GPU nodes

    last_toolbox_dir=$(ls ${ARTIFACT_DIR}/*__* -d | tail -1)
    echo "$1" > ${last_toolbox_dir}/EXPECTED_FAIL
}

prepare_cluster_for_gpu_operator() {
    ./run_toolbox.py cluster capture_environment

    finalizers+=("collect_must_gather")

    if ! ./toolbox/wdm test has_nfd_labels; then
        _expected_fail "Checking if the cluster had NFD labels"

        if ./toolbox/wdm test has_nfd_in_operatorhub; then
            ./toolbox/wdm ensure has_nfd_from_operatorhub
        else
            _warning "NFD_deployed_from_master" "NFD was deployed from master (not available in OperatorHub)"

            # install the NFD Operator from sources
            export CI_IMAGE_NFD_COMMIT_CI_REPO="${1:-https://github.com/openshift/cluster-nfd-operator.git}"
            export CI_IMAGE_NFD_COMMIT_CI_REF="${2:-master}"
            export CI_IMAGE_NFD_COMMIT_CI_IMAGE_TAG="ci-image"
            ./toolbox/wdm ensure has_nfd_from_master
        fi
    fi

    if ! ./toolbox/wdm test has_gpu_nodes; then
        _expected_fail "Checking if the cluster had GPU nodes"

        ./toolbox/wdm ensure has_gpu_nodes
    fi
}

collect_must_gather() {
    run_in_sub_shell() {
        echo "Running the GPU Operator must-gather image ..."
        OPERATOR_IMAGE=$(oc get pods -A -lapp=gpu-operator -o=jsonpath='{.items[0].spec.containers[0].image}' 2> /dev/null || true)

        TMP_DIR="$(mktemp -d -t gpu-operator_XXXX)"

        if [[ "$OPERATOR_IMAGE" ]]; then
            echo "Operator image: $OPERATOR_IMAGE"

            oc adm must-gather --image="$OPERATOR_IMAGE" --dest-dir="${TMP_DIR}" &> /dev/null

            # ${TMP_DIR}/<image>/ should contain the file generated by
            # the must-gather script. If this is empty, there wasn't a
            # must-gather script in the image!
            if [[ "$(ls "${TMP_DIR}"/*/* 2>/dev/null | wc -l)" == 0 ]]; then
                echo "GPU Operator image failed to must-gather anything ..."
            else
                img_dirname=$(dirname "$(ls "${TMP_DIR}"/*/* | head -1)")
                mv "$img_dirname"/* $TMP_DIR
                rmdir "$img_dirname"

                # extract ARTIFACT_EXTRA_LOGS_DIR from 'source toolbox/_common.sh' without sourcing it directly
                export TOOLBOX_SCRIPT_NAME=toolbox/gpu-operator/must-gather.sh
                COMMON_SH=$(source toolbox/_common.sh;
                            echo "8<--8<--8<--";
                            # only evaluate these variables from _common.sh
                            env | egrep "(^ARTIFACT_EXTRA_LOGS_DIR=)"
                         )
                ENV=$(echo "$COMMON_SH" | sed '0,/8<--8<--8<--/d') # keep only what's after the 8<--
                eval $ENV

                echo "Copying must-gather results to $ARTIFACT_EXTRA_LOGS_DIR ..."
                cp -r "$TMP_DIR"/* "$ARTIFACT_EXTRA_LOGS_DIR"

                rmdir "$TMP_DIR"
            fi
        else
            echo "Failed to find the GPU Operator image ..."
        fi

        # Calling this until we're sure that the GPU Operator
        # must-gather image captures all the information we need
        echo "Running gpu_operator capture_deployment_state ..."
        ./run_toolbox.py gpu_operator capture_deployment_state > /dev/null || true

        echo "Running gpu_operator capture_deployment_state ... done."

        version=$(cat "$ARTIFACT_DIR"/*__gpu_operator__capture_deployment_state/gpu_operator.version 2> /dev/null || echo MISSING)
        echo "$version" > ${ARTIFACT_DIR}/operator.version

        if [[ "$version" != "MISSING" ]]; then
            echo "Operator versions collected."
        else
            echo "Failed to collect the operator version ..."
        fi
    }

    # run the function above in a subshell to avoid polluting the local `env`.
    typeset -fx run_in_sub_shell
    bash -c run_in_sub_shell
}

validate_gpu_operator_deployment() {
    ./run_toolbox.py gpu_operator wait_deployment
    ./run_toolbox.py gpu_operator run_gpu_burn
}

cleanup_cluster() {
    # undeploy the entitlement
    ./run_toolbox.py entitlement undeploy
    # ensure that there is no GPU Operator in the cluster
    ./run_toolbox.py gpu_operator undeploy_from_operatorhub
    # ensure that there is no GPU node in the cluster
    ./run_toolbox.py cluster set_scale g4dn.xlarge 0

    # ensure that NFD is not installed in the cluster
    ## not working as expected at the moment: NFD labels remain
    ## visible, but new labels not added, making the
    ## `nfd__wait_gpu_nodes` test fail.
    #./run_toolbox.py nfd-operator undeploy_from_operatorhub

    # ensure that the MachineConfigPool have finished all pending updates
    tries_left=40
    WAIT_TIME_SECONDS=30
    set -x

    while true; do
        mcp_machine_count=$(oc get mcp -ojsonpath={.items[*].status.machineCount} | jq .)
        mcp_machine_updated=$(oc get mcp -ojsonpath={.items[*].status.updatedMachineCount} | jq .)
        if [ "$mcp_machine_count" == "$mcp_machine_updated" ]; then
            echo "All the MachineConfigPools have been updated."
            break
        fi
        tries_left=$(($tries_left - 1))
        if [[ $tries_left == 0 ]]; then
            cat <<EOF
Failed to wait for the MachineConfigPools to be properly updated.
machineCount:
$mcp_machine_count

updatedMachineCount:
$mcp_machine_updated
EOF
            oc get mcp > ${ARTIFACT_DIR}/mcp.list
            oc describe mcp > ${ARTIFACT_DIR}/mcp.all.descr
            exit 1
        fi
        sleep $WAIT_TIME_SECONDS
    done
}

publish_master_bundle() {
    trap collect_must_gather EXIT

    export CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID="master"
    deploy_commit "https://gitlab.com/nvidia/kubernetes/gpu-operator.git" "master"

    prepare_cluster_for_gpu_operator

    validate_gpu_operator_deployment
}

test_master_branch() {
    trap collect_must_gather EXIT

    deploy_commit "https://gitlab.com/nvidia/kubernetes/gpu-operator.git" "master"

    prepare_cluster_for_gpu_operator "$@"

    validate_gpu_operator_deployment
}

test_commit() {
    gpu_operator_git_repo="${1}"
    shift;
    gpu_operator_git_ref="${1}"
    shift;

    prepare_cluster_for_gpu_operator "$@"

    deploy_commit $gpu_operator_git_repo $gpu_operator_git_ref

    validate_gpu_operator_deployment
}

deploy_commit() {
    gpu_operator_git_repo="${1:-}"
    shift
    gpu_operator_git_ref="${1:-}"

    if [[ -z "$gpu_operator_git_repo" || -z "$gpu_operator_git_ref" ]]; then
        echo "FATAL: test_commit must receive a git repo/ref to be tested."
        return 1
    fi
    echo "Using Git repository ${gpu_operator_git_repo} with ref ${gpu_operator_git_ref}"

    OPERATOR_NAMESPACE="nvidia-gpu-operator"
    if [[ "${CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID:-}" ]]; then
        # use CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID when it's set
        true
    elif [[ "${JOB_NAME:-}" ]]; then
        # running in a CI job, use the job name
        CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID="ci-image-${JOB_NAME}"
    else
        echo "FATAL: test_commit expects CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID or JOB_NAME to be defined."
        return 1
    fi

    GPU_OPERATOR_QUAY_BUNDLE_PUSH_SECRET=${GPU_OPERATOR_QUAY_BUNDLE_PUSH_SECRET:-"/var/run/psap-entitlement-secret/openshift-psap-openshift-ci-secret.yml"}
    GPU_OPERATOR_QUAY_BUNDLE_IMAGE_NAME=${GPU_OPERATOR_QUAY_BUNDLE_IMAGE_NAME:-"quay.io/openshift-psap/ci-artifacts"}

    ./run_toolbox.py gpu_operator bundle_from_commit "${gpu_operator_git_repo}" \
                                             "${gpu_operator_git_ref}" \
                                             "${GPU_OPERATOR_QUAY_BUNDLE_PUSH_SECRET}" \
                                             "${GPU_OPERATOR_QUAY_BUNDLE_IMAGE_NAME}" \
                                             --tag_uid "${CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID}" \
                                             --namespace "${OPERATOR_NAMESPACE}" \
                                             --with_validator=true \
                                             --publish_to_quay=true

    ./run_toolbox.py gpu_operator deploy_from_bundle --bundle "${GPU_OPERATOR_QUAY_BUNDLE_IMAGE_NAME}:operator_bundle_gpu-operator-${CI_IMAGE_GPU_COMMIT_CI_IMAGE_UID}" \
                                                     --namespace "${OPERATOR_NAMESPACE}"
}

test_operatorhub() {
    OPERATOR_NAMESPACE="nvidia-gpu-operator"

    if [ "${1:-}" ]; then
        OPERATOR_VERSION="--version=$1"
    fi
    shift || true
    if [ "${1:-}" ]; then
        OPERATOR_CHANNEL="--channel=$1"
        if [[ "${OPERATOR_CHANNEL}" != *"1.9"* ]]; then
            OPERATOR_NAMESPACE="openshift-operators"
        fi
    fi
    shift || true

    prepare_cluster_for_gpu_operator "$@"

    ./run_toolbox.py gpu_operator deploy_from_operatorhub \
                     ${OPERATOR_CHANNEL:-} \
                     ${OPERATOR_VERSION:-} \
                     --namespace ${OPERATOR_NAMESPACE}

    validate_gpu_operator_deployment
}

validate_deployment_post_upgrade() {
    finalizers+=("collect_must_gather")
    finalizers+=("./run_toolbox.py entitlement undeploy &> /dev/null")

    validate_gpu_operator_deployment
}

finalizers=()
run_finalizers() {
    [ ${#finalizers[@]} -eq 0 ] && return
    set +x

    echo "Running exit finalizers ..."
    for finalizer in "${finalizers[@]}"
    do
        echo "Running finalizer '$finalizer' ..."
        eval $finalizer
    done
}

if [ -z "${1:-}" ]; then
    echo "FATAL: $0 expects at least 1 argument ..."
    exit 1
fi

trap run_finalizers EXIT

action="$1"
shift

if [[ "${action}" != "source" ]]; then
    set -x
fi

case ${action} in
    "test_master_branch")
        test_master_branch "$@"
        exit 0
        ;;
    "test_commit")
        test_commit "https://gitlab.com/nvidia/kubernetes/gpu-operator.git" master "$@"
        exit 0
        ;;
    "test_operatorhub")
        test_operatorhub "$@"
        exit 0
        ;;
    "validate_deployment_post_upgrade")
        validate_gpu_operator_deployment
        exit 0
        ;;
    "publish_master_bundle")
        publish_master_bundle
        exit 0
        ;;
    "cleanup_cluster")
        cleanup_cluster
        exit 0
        ;;
    "source")
        # file is being sourced by another script
        echo "INFO: GPU Operator CI entrypoint has been sourced"
        ;;
    *)
        echo "FATAL: Unknown action: ${action}" "$@"
        exit 1
        ;;
esac
