---
- name: Ensure that NFD is running
  command: oc get nodes -lfeature.node.kubernetes.io/system-os_release.ID=rhcos

- name: Ensure that NFD found nodes with GPU labels
  # label list should be in sync with:
  # https://github.com/NVIDIA/gpu-operator/blob/master/pkg/controller/clusterpolicy/state_manager.go#L26
  shell:
    (   oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-10de.present
     || oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-0302_10de.present
     || oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-0300_10de.present
    ) | grep .

- name: Ensure that the ClusterPolicy CRD is deployed
  command: oc get crds/clusterpolicies.nvidia.com
  register: has_clusterpolicy_crd
  until:
  - has_clusterpolicy_crd.rc == 0
  retries: 10
  delay: 10

- name: Ensure that the ClusterPolicy CR is deployed
  command: oc get ClusterPolicies -oname

- name: Ensure that nvidia-device-plugin-validation Pod has ran successfully
  shell:
    oc get pods --field-selector=status.phase=Succeeded
                -n gpu-operator-resources
                -oname
                --no-headers 2> /dev/null
   | grep nvidia-device-plugin-validation
  register: has_deviceplugin_validation_pod
  until:
  - has_deviceplugin_validation_pod.stdout == "pod/nvidia-device-plugin-validation"
  retries: 15
  delay: 60

- block:
  - name: Ensure that the gpu-feature-discovery Pod has labeled the nodes
    shell: oc get nodes -l nvidia.com/gpu.count -oname
    register: nv_gpu_feature_discovery_check
    until:
    - nv_gpu_feature_discovery_check.stdout != ""
    retries: 10
    delay: 30
  rescue:
  - name: Capture the GFD logs (debug)
    shell: "oc logs ds/gpu-feature-discovery -n gpu-operator-resources > {{ artifact_extra_logs_dir }}/gpu_gfd_logs"
    ignore_errors: true
  - fail:
      msg: The GFD did not label the nodes

- block:
  - name: Ensure that the nvidia-dcgm-exporter Pod is responding appropriately
    shell: |
      DCGM_POD=$(oc get pods -lapp=nvidia-dcgm-exporter -oname -n gpu-operator-resources | head -1);
      if [ -z "$DCGM_POD" ]; then
        echo "Failed to find a pod for nvidia-dcgm-exporter";
        exit 10;
      fi;
      DCGM_PORT=9400; LOCAL_PORT=9401;
      retry=5;
      timeout 10 oc port-forward ${DCGM_POD} ${LOCAL_PORT}:${DCGM_PORT} -n gpu-operator-resources &
      while [ "$DCGM_OUTPUT" == "" ]; do
        sleep 1;
        DCGM_OUTPUT=$(curl localhost:${LOCAL_PORT}/metrics 2>/dev/null);
        retry=$(($retry - 1));
        if [[ $retry == 0 ]]; then
          echo "Failed to get any output from DCGM/metrics ...";
          exit 11;
        fi;
      done;
      exec grep "# TYPE DCGM_FI_DEV" <<< ${DCGM_OUTPUT}
    register: nv_dcgm_exporter_check
    until:
    - nv_dcgm_exporter_check.rc == 0
    retries: 10
    delay: 20

  - name: Ensure that Prometheus picked up the DCGM endpoint
    shell:
      oc get secret prometheus-k8s -n openshift-monitoring -ojson | jq -r '.data["prometheus.yaml.gz"]'
      | base64 -d
      | gunzip
      | grep dcgm
    register: nv_dcgm_exporter_prom
    ignore_errors: true
    until: nv_dcgm_exporter_prom.rc == 0
    retries: 5
    delay: 20

  rescue:
  - name: Capture the DCGM logs (debug)
    shell: "oc logs ds/nvidia-dcgm-exporter -n gpu-operator-resources > {{ artifact_extra_logs_dir }}/gpu_dcgm_logs"
    when: artifact_extra_logs_dir | default('', true) | trim != ''
    ignore_errors: true
  - fail:
      msg: The DCGM does not expose metrics
