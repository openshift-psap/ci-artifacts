---
- name: Ensure that NFD found nodes with GPU labels
  # label list should be in sync with:
  # https://github.com/NVIDIA/gpu-operator/blob/master/pkg/controller/clusterpolicy/state_manager.go#L26
  shell:
    set -o pipefail;
    (   oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-10de.present
     || oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-0302_10de.present
     || oc get nodes -oname --ignore-not-found=false -l feature.node.kubernetes.io/pci-0300_10de.present
    ) | grep .

- name: Get the list of nodes with GPUs
  shell:
    set -o pipefail;
    oc get nodes
       -lnvidia.com/gpu.present=true
       -o custom-columns=NAME:metadata.name
       --no-headers
    | cut -d. -f1
  register: gpu_burn_gpu_nodes
  failed_when: gpu_burn_gpu_nodes.stdout == ""

- name: Create the entrypoint ConfigMap
  command: oc apply -f "{{ gpu_burn_cm_entrypoint }}"

- name: Delete possibly stalled GPU burn Pods
  command: oc --ignore-not-found=true delete pod/gpu-burn-{{ item }} -n default
  with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
  failed_when: false

- name: Create GPU burn Pods
  with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
  shell: |
    set -eo pipefail;
    GPU_NODE_HOSTNAME=$(echo '{{ item }}' );
    echo "Hostname: $GPU_NODE_HOSTNAME";
    oc --ignore-not-found=true delete pod/gpu-burn-$GPU_NODE_HOSTNAME -n default
    cat "{{ gpu_burn_pod }}" \
      | sed "s|{{ '{{' }} gpu_node_hostname {{ '}}' }}|$GPU_NODE_HOSTNAME|" \
      | sed "s|{{ '{{' }} gpu_burn_time {{ '}}' }}|{{ gpu_burn_time }}|" \
      | oc apply -f-

- name: "Let the GPU burn Pods run {{ gpu_burn_time }}seconds"
  command: "sleep {{ gpu_burn_time }}"

- name: Ensure that the GPU burn ran successfully
  block:
  - name: Wait for GPU burn Pods to complete
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    command:
      oc get pod/gpu-burn-{{ item }}
         -n default
         -o custom-columns=:.status.phase
         --no-headers
    register: gpu_burn_wait
    until: gpu_burn_wait.stdout == "Succeeded" or gpu_burn_wait.stdout == "Error" or gpu_burn_wait.stdout == "Failed"
    failed_when: gpu_burn_wait.stdout != "Succeeded"
    retries: 10
    delay: 30

  - name: Ensure that no GPU was faulty
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    shell:
      oc logs pod/gpu-burn-{{ item }} -n default | grep FAULTY
    register: gpu_burn_test_faulty
    failed_when: gpu_burn_test_faulty.rc == 0

  always:
  - name: Save the logs of the GPU burn Pods
    shell: oc logs pod/gpu-burn-{{ item }} -n default | grep -o "[^$(printf '\r')]*$"
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Save the description of the GPU burn Pods
    shell: oc describe pod/gpu-burn-{{ item }} -n default > {{ artifact_extra_logs_dir }}/gpu_burn.{{ item }}.description.txt
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Save the full logs of the GPU burn Pods
    shell: oc logs pod/gpu-burn-{{ item }} -n default > {{ artifact_extra_logs_dir }}/gpu_burn.{{ item }}.log
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Cleanup the GPU burn Pods
    command: oc --ignore-not-found=true delete pod/gpu-burn-{{ item }} -n default
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Delete the entrypoint ConfigMap
    command: oc --ignore-not-found=true delete -f "{{ gpu_burn_cm_entrypoint }}"
    failed_when: false
