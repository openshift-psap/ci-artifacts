---
- name: Ensure that NFD found nodes with GPU labels
  # label list should be in sync with:
  # https://github.com/NVIDIA/gpu-operator/blob/master/pkg/controller/clusterpolicy/state_manager.go#L26
  shell:
    set -o pipefail;
    (   oc get nodes -oname -l feature.node.kubernetes.io/pci-10de.present
     || oc get nodes -oname -l feature.node.kubernetes.io/pci-0302_10de.present
     || oc get nodes -oname -l feature.node.kubernetes.io/pci-0300_10de.present
    ) | grep .

- name: Get the list of nodes with GPUs
  command:
    oc get nodes
       -lnvidia.com/gpu.present=true
       -o custom-columns=NAME:metadata.name
       --no-headers
  register: gpu_burn_gpu_nodes
  failed_when: gpu_burn_gpu_nodes.stdout == ""

- name: Create the entrypoint ConfigMap
  command: oc apply -f "{{ gpu_burn_cm_entrypoint }}"

- name: Delete the src ConfigMap, if it exists
  command:
    oc delete cm/gpu-burn-src
       --ignore-not-found=true
       -n default

- name: Create the src ConfigMap
  command:
    oc create configmap gpu-burn-src
       --from-file={{ gpu_burn_src_dir }}
       -n default

- name: Delete possibly stalled GPU burn Pods
  command: oc --ignore-not-found=true delete pod/gpu-burn-{{ item }} -n default
  with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
  failed_when: false

- name: Instanciate GPU Burn Pods from template
  loop: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
  loop_control:
    loop_var: gpu_node_name
  template:
    src: "{{ gpu_burn_pod }}"
    dest: "{{ artifact_extra_logs_dir }}/gpu_burn.{{ gpu_node_name }}.yml"
    mode: 0400

- name: Create GPU Burn Pods from template
  loop: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
  command: oc apply -f "{{ artifact_extra_logs_dir }}/gpu_burn.{{ item }}.yml"

- name: "Let the GPU burn Pods run {{ gpu_burn_time }} seconds"
  command: "sleep {{ gpu_burn_time }}"

- name: Ensure that the GPU burn ran successfully
  block:
  - name: Wait for GPU burn Pods to complete
    loop: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    command:
      oc get pod/gpu-burn-{{ item }}
         -n default
         -o custom-columns=:.status.phase
         --no-headers
    register: gpu_burn_wait
    until: gpu_burn_wait.stdout == "Succeeded" or gpu_burn_wait.stdout == "Error" or gpu_burn_wait.stdout == "Failed"
    failed_when: gpu_burn_wait.stdout != "Succeeded"
    retries: 20
    delay: 30

  - name: Ensure that no GPU was faulty
    loop: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    shell:
      oc logs pod/gpu-burn-{{ item }} -n default | grep FAULTY
    register: gpu_burn_test_faulty
    failed_when: gpu_burn_test_faulty.rc == 0

  always:
  - name: Save the logs of the GPU burn Pods
    shell: oc logs pod/gpu-burn-{{ item }} -n default | grep -o "[^$(printf '\r')]*$"
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Save the description of the GPU burn Pods
    shell: oc describe pod/gpu-burn-{{ item }} -n default > {{ artifact_extra_logs_dir }}/gpu_burn.{{ item }}.description.txt
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Save the full logs of the GPU burn Pods
    shell: oc logs pod/gpu-burn-{{ item }} -n default > {{ artifact_extra_logs_dir }}/gpu_burn.{{ item }}.log
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Cleanup the GPU burn Pods
    command: oc --ignore-not-found=true delete pod/gpu-burn-{{ item }} -n default
    with_items: "{{ gpu_burn_gpu_nodes.stdout_lines }}"
    failed_when: false

  - name: Delete the entrypoint ConfigMap
    command: oc --ignore-not-found=true delete -f "{{ gpu_burn_cm_entrypoint }}"
    failed_when: false
