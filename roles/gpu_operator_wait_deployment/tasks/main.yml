---
- name: Ensure that NFD is deployed
  include_role:
    name: nfd_has_labels

- name: Ensure that there are GPU nodes
  include_role:
    name: nfd_test_wait_gpu

- name: Wait for the ClusterPolicy CRD to be deployed
  command: oc get crds/clusterpolicies.nvidia.com
  register: has_clusterpolicy_crd
  until:
  - has_clusterpolicy_crd.rc == 0
  retries: 10
  delay: 10

- name: Wait for the ClusterPolicy CR to be deployed
  command: oc get ClusterPolicies -oname
  register: has_clusterpolicy
  until:
  - has_clusterpolicy.rc == 0
  retries: 10
  delay: 10

- name: Wait for the GPU Operator namespace (only >= v1.7.0)
  when: gpu_operator_version is version("1.7.0", ">=")
  # With the GPU Operator >=v1.7.0, the namespace is created by the
  # operator, so it takes a few seconds before being available after the
  # deployment.
  command: oc get ns/gpu-operator-resources
  register: has_gpu_operator_namespace
  until: has_gpu_operator_namespace.rc == 0
  retries: 10
  delay: 10

- name: Wait for the GPU Operator (>= v1.7.0) to run its internal validation steps
  when: gpu_operator_version is version("1.7.0", ">=")
  block:
  - name: Ensure that nvidia-operator-validator DS is ready
    command:
      oc get ds/nvidia-operator-validator
         -n gpu-operator-resources
         -oyaml
         -ojsonpath={.status.numberUnavailable}
    register: operator_validator_numberUnavailable
    until:
    - operator_validator_numberUnavailable.rc == 0
    - not operator_validator_numberUnavailable.stdout
    retries: 15
    delay: 60

- name: Wait for the GPU Operator (< v1.7.0) to run its internal validation steps
  when: gpu_operator_version is version("1.7.0", "<")
  block:
  - name: Ensure that nvidia-device-plugin-validation Pod has ran successfully
    command:
      oc get pods
        --field-selector=metadata.name=nvidia-device-plugin-validation,status.phase=Succeeded
        -n gpu-operator-resources
        -oname --no-headers
    register: has_deviceplugin_validation_pod
    until:
    - has_deviceplugin_validation_pod.stdout == "pod/nvidia-device-plugin-validation"
    retries: 15
    delay: 60

- block:
  - name: Wait for the gpu-feature-discovery Pod to label the nodes
    command: oc get nodes -l nvidia.com/gpu.count -oname
    register: has_gpu_feature_discovery_labels
    until:
    - has_gpu_feature_discovery_labels.stdout != ""
    retries: 20
    delay: 30

  rescue:
  - name: Capture the GFD logs (debug)
    shell:
      oc logs ds/gpu-feature-discovery
         -n gpu-operator-resources > {{ artifact_extra_logs_dir }}/gpu_operator_gfd.log
    failed_when: false

  - name: The GFD did not label the nodes
    fail: msg="The GFD did not label the nodes"

- block:
  - name: Check if the namespace has the openshift.io/cluster-monitoring label
    shell: oc get ns -l openshift.io/cluster-monitoring -oname | grep gpu-operator-resources
  rescue:
  - name: Get the namespace yaml specification
    command: oc get ns/gpu-operator-resources -oyaml

  - name: Make sure that namespace has the openshift.io/cluster-monitoring label
    command: oc label ns/gpu-operator-resources openshift.io/cluster-monitoring=true

- name: Validate that the DCGM metrics are correctly exposed
  block:
  - name: Wait for the nvidia-dcgm-exporter Pod to respond appropriately
    command:
      bash "{{ gpu_operator_fetch_pod_metrics_script }}" \
           9400 app=nvidia-dcgm-exporter gpu-operator-resources \
           '# HELP'
    register: dcgm_exporter_check
    until:
    - dcgm_exporter_check.rc == 0
    retries: 10
    delay: 20

  - name: Wait for Prometheus to pick up the GPU Operator DCGM target
    shell:
      set -o pipefail;
      oc get secret prometheus-k8s -n openshift-monitoring -ojson | jq -r '.data["prometheus.yaml.gz"]'
      | base64 -d
      | gunzip
      | grep dcgm
    register: dcgm_exporter_prom
    until: dcgm_exporter_prom.rc == 0
    retries: 30
    delay: 30

  rescue:
  - name: Capture the DCGM logs (debug)
    shell:
      oc logs ds/nvidia-dcgm-exporter
         -n gpu-operator-resources > {{ artifact_extra_logs_dir }}/gpu_operator_dcgm.log
    failed_when: false

  - name: The GPU Operator does not correctly expose its DCGM metrics
    fail: msg="The GPU Opertor does not correctly expose the its DCGM metrics to Prometheus"
